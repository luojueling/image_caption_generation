{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "from models import Encoder, DecoderWithRNN, DecoderWithAttention\n",
    "from datasets import *\n",
    "from solver import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    # Data parameters\n",
    "    'data_folder' : '/data3/zhangweiyi/coco2014',  # folder with data files saved by create_input_files.py\n",
    "    'data_name' : 'coco_5_cap_per_img_5_min_word_freq',  # base name shared by data files\n",
    "    # Model parameters\n",
    "    'embed_dim' : 512,  # dimension of word embeddings\n",
    "    'attention_dim' : 512,  # dimension of attention linear layers\n",
    "    'decoder_dim' : 512,  # dimension of decoder RNN\n",
    "    'dropout' : 0.5,\n",
    "    'device' : torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),  # sets device for model and PyTorch tensors\n",
    "    # Training parameters\n",
    "    'start_epoch' : 0,\n",
    "    'epochs' : 10,  # number of epochs to train for (if early stopping is not triggered)\n",
    "    'epochs_since_improvement' : 0,  # keeps track of number of epochs since there's been an improvement in validation BLEU\n",
    "    'batch_size' : 32,\n",
    "    'workers' : 1,  # for data-loading; right now, only 1 works with h5py\n",
    "    'encoder_lr' : 1e-4,  # learning rate for encoder if fine-tuning\n",
    "    'decoder_lr' : 4e-4,  # learning rate for decoder\n",
    "    'grad_clip' : 5.,  # clip gradients at an absolute value of\n",
    "    'alpha_c' : 1.,  # regularization parameter for 'doubly stochastic attention', as in the paper\n",
    "    'best_bleu4' : 0.,  # BLEU-4 score right now\n",
    "    'print_freq' : 100,  # print training/validation stats every __ batches\n",
    "    'fine_tune_encoder' : False,  # fine-tune encoder or not\n",
    "    'checkpoint' : None,  # path to checkpoint, None if none\n",
    "    'attention' : False, # train decoder with attention or not\n",
    "}\n",
    "cudnn.benchmark = True  # set to true only if inputs to model are fixed size; otherwise lot of computational overhead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_map_file = os.path.join(cfg['data_folder'], 'WORDMAP_' + cfg['data_name'] + '.json')\n",
    "with open(word_map_file, 'r') as j:\n",
    "    word_map = json.load(j)\n",
    "cfg['vocab_size'] = len(word_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg['checkpoint'] is None:\n",
    "    encoder = Encoder()\n",
    "    encoder.fine_tune(cfg['fine_tune_encoder'])\n",
    "    encoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, encoder.parameters()),\n",
    "                                         lr=cfg['encoder_lr']) if cfg['fine_tune_encoder'] else None\n",
    "    if not cfg['attention']:\n",
    "        decoder = DecoderWithRNN(cfg)\n",
    "    else:\n",
    "        decoder = DecoderWithAttention(cfg)\n",
    "    decoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, decoder.parameters()),\n",
    "                                         lr=cfg['decoder_lr'])\n",
    "else:\n",
    "    checkpoint = torch.load(cfg['checkpoint'])\n",
    "    cfg['start_epoch'] = checkpoint['epoch'] + 1\n",
    "    cfg['epochs_since_improvement'] = checkpoint['epochs_since_improvement']\n",
    "    cfg['best_bleu4'] = checkpoint['bleu-4']\n",
    "    encoder = checkpoint['encoder']\n",
    "    encoder_optimizer = checkpoint['encoder_optimizer']\n",
    "    decoder = checkpoint['decoder']\n",
    "    decoder_optimizer = checkpoint['decoder_optimizer']\n",
    "    if cfg['fine_tune_encoder'] is True and encoder_optimizer is None:\n",
    "        encoder.fine_tune(cfg['fine_tune_encoder'])\n",
    "        encoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, encoder.parameters()),\n",
    "                                             lr=cfg['encoder_lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move to GPU, if available\n",
    "decoder = decoder.to(cfg['device'])\n",
    "encoder = encoder.to(cfg['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss().to(cfg['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataloaders\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    CaptionDataset(cfg['data_folder'], cfg['data_name'], 'TRAIN', transform=transforms.Compose([normalize])),\n",
    "    batch_size=cfg['batch_size'], shuffle=True, num_workers=cfg['workers'], pin_memory=True)\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    CaptionDataset(cfg['data_folder'], cfg['data_name'], 'VAL', transform=transforms.Compose([normalize])),\n",
    "    batch_size=cfg['batch_size'], shuffle=True, num_workers=cfg['workers'], pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epochs\n",
    "for epoch in range(cfg['start_epoch'], cfg['epochs']):\n",
    "\n",
    "    # Decay learning rate if there is no improvement for 8 consecutive epochs, and terminate training after 20\n",
    "    if cfg['epochs_since_improvement'] == 20:\n",
    "        break\n",
    "    if cfg['epochs_since_improvement'] > 0 and cfg['epochs_since_improvement'] % 8 == 0:\n",
    "        adjust_learning_rate(decoder_optimizer, 0.8)\n",
    "        if cfg['fine_tune_encoder']:\n",
    "            adjust_learning_rate(encoder_optimizer, 0.8)\n",
    "\n",
    "    # One epoch's training\n",
    "    train(train_loader=train_loader,\n",
    "          encoder=encoder,\n",
    "          decoder=decoder,\n",
    "          criterion=criterion,\n",
    "          encoder_optimizer=encoder_optimizer,\n",
    "          decoder_optimizer=decoder_optimizer,\n",
    "          epoch=epoch,\n",
    "          cfg=cfg)\n",
    "    \n",
    "    # One epoch's validation\n",
    "    recent_bleu4 = validate(val_loader=val_loader,\n",
    "                            encoder=encoder,\n",
    "                            decoder=decoder,\n",
    "                            criterion=criterion,\n",
    "                            word_map=word_map,\n",
    "                            cfg=cfg)\n",
    "\n",
    "    # Check if there was an improvement\n",
    "    is_best = recent_bleu4 > cfg['best_bleu4']\n",
    "    cfg['best_bleu4'] = max(recent_bleu4, cfg['best_bleu4'])\n",
    "    if not is_best:\n",
    "        cfg['epochs_since_improvement'] += 1\n",
    "        print(\"\\nEpochs since last improvement: %d\\n\" % (cfg['epochs_since_improvement'],))\n",
    "    else:\n",
    "        cfg['epochs_since_improvement'] = 0\n",
    "\n",
    "    # Save checkpoint\n",
    "    save_checkpoint(cfg['data_name'], epoch, cfg['epochs_since_improvement'], encoder, decoder, encoder_optimizer,\n",
    "                    decoder_optimizer, recent_bleu4, is_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
